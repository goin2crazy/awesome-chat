{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Load all required libs ","metadata":{}},{"cell_type":"code","source":"! pip install -q --upgrade pip\n! pip install -q transformers[torch]\n! pip install -q -U transformers==4.38.2 datasets==2.18.0 evaluate rouge_score peft\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\nos.environ[\"HF_TOKEN\"] = UserSecretsClient().get_secret(\"HF_TOKEN\")\n\ntry: \n    import wandb\n    wandb.init(mode='disabled')\nexcept: \n    ...\n    \n# Check the datasets lib version\n# It have to be the latest v.\nimport datasets \n\nprint(datasets.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-16T10:06:54.468392Z","iopub.execute_input":"2024-04-16T10:06:54.468821Z","iopub.status.idle":"2024-04-16T10:08:11.581964Z","shell.execute_reply.started":"2024-04-16T10:06:54.468786Z","shell.execute_reply":"2024-04-16T10:08:11.580846Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"2.18.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load all models","metadata":{}},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport torch\nfrom torch.nn import DataParallel\n\nclass SumModel():\n  def __init__(cfg, model_preset, **generation_parameters)->None:\n    # requires transformers and peft installed libs\n\n    cfg.model_preset = model_preset\n    cfg.generation_params = generation_parameters\n\n    config = PeftConfig.from_pretrained(cfg.model_preset)\n\n    model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n    cfg.tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n\n    cfg.lora_model = PeftModel.from_pretrained(model, cfg.model_preset)\n\n    cfg.lora_model.print_trainable_parameters()\n\n    cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    cfg.lora_model.model = DataParallel(cfg.lora_model.model)\n\n  def __call__(self, text, **generation_params):\n    tokens = self.tokenizer(text, return_tensors = 'pt', truncation=True, padding=True).to(self.device)\n    if len(generation_params):\n      gen = self.lora_model.generate(**tokens, **generation_params)\n    else:\n      gen = self.lora_model.generate(**tokens, **self.generation_params)\n\n    return self.tokenizer.batch_decode(gen)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:17:28.782012Z","iopub.execute_input":"2024-04-16T10:17:28.782383Z","iopub.status.idle":"2024-04-16T10:17:28.791662Z","shell.execute_reply.started":"2024-04-16T10:17:28.782355Z","shell.execute_reply":"2024-04-16T10:17:28.790506Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"from peft import PeftModel, PeftConfig\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\npygmalion_config = PeftConfig.from_pretrained(\"doublecringe123/pygmalion-dialogsum-empathetic_dialogues_llm\")\n\npygmalion_model = AutoModelForCausalLM.from_pretrained(pygmalion_config.base_model_name_or_path)\npygmalion_tokenizer = AutoTokenizer.from_pretrained(pygmalion_config.base_model_name_or_path)\n\npygmalion_model = PeftModel.from_pretrained(pygmalion_model, \"doublecringe123/pygmalion-dialogsum-empathetic_dialogues_llm\")","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:08:40.849893Z","iopub.execute_input":"2024-04-16T10:08:40.850754Z","iopub.status.idle":"2024-04-16T10:09:42.928005Z","shell.execute_reply.started":"2024-04-16T10:08:40.850718Z","shell.execute_reply":"2024-04-16T10:09:42.926566Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/445 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b86ecd8c1f487eadd5ff45b6313b7b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.49k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"536bdb4071dc443a998aca083bad70b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/5.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c18b2dfe8c7a428fb0071cd587c171f8"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/717 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16587044877b4fdea28b6401e8eefafb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6cc1916dbab9454b9105334d726544e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"728a92edd5b64335b22169edabdaedbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d01288be21e145e6b746393050fe176e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8d673b4f326e4317a43c1e4ac2c55c98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/205k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f93aed4ffad4378b2da3d5bac2cce39"}},"metadata":{}}]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nimport torch\nfrom torch.nn import DataParallel\n\nclass ChatModel():\n    @staticmethod\n    def join_dialogue(d):\n      s = \"\"\n\n      for message in d:\n        if message['role'] == 'user':\n          s += f\"You: {message['content']}\\n\"\n        else:\n          s += f\"[CHARACTER]: {message['content']}\\n\"\n\n      return s\n\n    def __init__(cfg, tokenizer, model, chat_summarization_fn, **generation_parameters)->None:\n        # requires transformers and peft installed libs\n        cfg.tokenizer = tokenizer\n        \n        cfg.summ = chat_summarization_fn\n        cfg.generation_params = generation_parameters\n        \n        cfg.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        cfg.model = model.to(cfg.device)\n\n    def __call__(self, d, messages_split:int = 6, **generation_params):\n        print(\"0/1 Summarization process..\")\n        s_text = self.join_dialogue(d[:-messages_split])\n        s = self.summ(s_text)\n        \n        text = f\"*HISTORY: {s}* {self.join_dialogue(d[-messages_split:])}\"\n        print(f\"1/1 Summarized Dialogue: {text}\")\n        \n        tokens = self.tokenizer(text, return_tensors = 'pt', truncation=True, padding=True).to(self.device)\n        if len(generation_params):\n            gen = self.model.generate(**tokens, **generation_params)\n        else:\n            gen = self.model.generate(**tokens, **self.generation_params)\n\n        return self.tokenizer.batch_decode(gen)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:36:46.614444Z","iopub.execute_input":"2024-04-16T10:36:46.614951Z","iopub.status.idle":"2024-04-16T10:36:46.626349Z","shell.execute_reply.started":"2024-04-16T10:36:46.614899Z","shell.execute_reply":"2024-04-16T10:36:46.625069Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"sum_model = SumModel(model_preset = 'doublecringe123/bardt-large-cnn-dialoguesum-booksum-lora',\n                 max_length = 96,\n                 min_length = 26,\n                 do_sample = True,\n                 temperature = 0.9,\n                 num_beams = 8,\n                 repetition_penalty= 2.)\nsumm = lambda t: sum_model(t)[0]\n\ngeneration_params = {\n    'max_new_tokens' : 50, \n    'do_sample' : True, \n    'temperature' : 0.9, \n    'repetition_penalty': 2.\n}\n\nchat_model = ChatModel(tokenizer = pygmalion_tokenizer, \n                       model = pygmalion_model, \n                       chat_summarization_fn = summ,  \n                       **generation_params)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:36:50.644925Z","iopub.execute_input":"2024-04-16T10:36:50.645761Z","iopub.status.idle":"2024-04-16T10:36:53.613032Z","shell.execute_reply.started":"2024-04-16T10:36:50.645720Z","shell.execute_reply":"2024-04-16T10:36:53.611682Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"trainable params: 0 || all params: 408,059,904 || trainable%: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"d = [\n    {\"content\": \"I just tried this new coffee blend, and it's amazing!\", \"role\": \"user\"},\n    {\"content\": \"That's great to hear! What did you like most about it?\", \"role\": \"bot\"},\n    {\"content\": \"The flavor was so rich and smooth, unlike anything I've tasted before.\", \"role\": \"user\"},\n    {\"content\": \"Sounds like a high-quality blend. Did it have any specific notes or undertones?\", \"role\": \"bot\"},\n    {\"content\": \"Yes, it had a hint of chocolate and caramel, which made it really indulgent.\", \"role\": \"user\"},\n    {\"content\": \"Interesting! I love coffees with those flavor profiles. Where did you get it from?\", \"role\": \"bot\"},\n    {\"content\": \"I got it from a local roastery that specializes in unique blends.\", \"role\": \"user\"},\n    {\"content\": \"Supporting local businesses and enjoying great coffee, what a win-win!\", \"role\": \"bot\"},\n    {\"content\": \"Definitely! I'm always on the lookout for new coffee experiences.\", \"role\": \"user\"},\n    {\"content\": \"Have you tried any other interesting blends recently?\", \"role\": \"bot\"},\n    {\"content\": \"Not yet, but I'm planning to explore more options soon. Any recommendations?\", \"role\": \"user\"},\n    {\"content\": \"I recently tried a Ethiopian Yirgacheffe that had floral and fruity notes, really refreshing.\", \"role\": \"bot\"},\n    {\"content\": \"That sounds amazing! I'll add it to my list of coffees to try. Thanks for the suggestion!\", \"role\": \"user\"},\n    {\"content\": \"You're welcome! Enjoy your coffee adventures!\", \"role\": \"bot\"},\n    {\"content\": \"I definitely will. Coffee is my passion!\", \"role\": \"user\"},\n    {\"content\": \"It's always great to meet a fellow coffee enthusiast. Cheers to delicious brews!\", \"role\": \"bot\"},\n    {\"content\": \"Cheers!\", \"role\": \"user\"},\n    {\"content\": \"Have a wonderful day!\", \"role\": \"bot\"},\n    {\"content\": \"You too!\", \"role\": \"user\"}\n]\n\nchat_model(d)","metadata":{"execution":{"iopub.status.busy":"2024-04-16T10:36:55.861660Z","iopub.execute_input":"2024-04-16T10:36:55.862101Z","iopub.status.idle":"2024-04-16T10:56:57.189895Z","shell.execute_reply.started":"2024-04-16T10:36:55.862067Z","shell.execute_reply":"2024-04-16T10:56:57.188965Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"0/1 Summarization process..\n","output_type":"stream"},{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"1/1 Summarized Dialogue: *HISTORY: </s><s><s>. You tells [CHARACTER] that You just tried a new coffee blend with a hint of chocolate and caramel from a local roastery. You's always on the lookout for new coffee experiences.</s>* [CHARACTER]: You're welcome! Enjoy your coffee adventures!\nYou: I definitely will. Coffee is my passion!\n[CHARACTER]: It's always great to meet a fellow coffee enthusiast. Cheers to delicious brews!\nYou: Cheers!\n[CHARACTER]: Have a wonderful day!\nYou: You too!\n\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/peft_model.py:1232: UserWarning: Position ids are not supported for parameter efficient tuning. Ignoring position ids.\n  warnings.warn(\"Position ids are not supported for parameter efficient tuning. Ignoring position ids.\")\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[\"*HISTORY: </s><s><s>. You tells [CHARACTER] that You just tried a new coffee blend with a hint of chocolate and caramel from a local roastery. You's always on the lookout for new coffee experiences.</s>* [CHARACTER]: You're welcome! Enjoy your coffee adventures!\\nYou: I definitely will. Coffee is my passion!\\n[CHARACTER]: It's always great to meet a fellow coffee enthusiast. Cheers to delicious brews!\\nYou: Cheers!\\n[CHARACTER]: Have a wonderful day!\\nYou: You too!\\n[CHARACTER]: That must be very fine, what makes him? What was like our work out he's still enjoy it may sound different things about his first drink in them are so -le at 12 weeks...ating gets here that when you make me or good thing to get yourself. *at least we can't forget this time I am not happy.* That sounds nice but even their drinks all of people love as they don't know her food are good deal. The best drinks, where an espresso, thanks you have never been going drinking. 3 such M m n<|endoftext|>\"]"},"metadata":{}}]},{"cell_type":"markdown","source":"# need to more training :D","metadata":{}}]}